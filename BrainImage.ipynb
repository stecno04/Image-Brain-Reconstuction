{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from mayavi import mlab\n",
    "import gzip\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanSquaredLogarithmicError, CosineSimilarity, LogCoshError\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(gzipped_file_path):\n",
    "    with gzip.open(gzipped_file_path, 'rb') as f_in:\n",
    "        unzipped_file_path = gzipped_file_path.replace('.gz', '')\n",
    "        with open(unzipped_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    # Delete the original gzipped file\n",
    "    os.remove(gzipped_file_path)\n",
    "    \n",
    "    return unzipped_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ogni cartella e per ogni files all'interno unzip\n",
    "for root, dirs, files in os.walk(\"data\\sub-01\"):\n",
    "    for file in tqdm(files, desc=\"Processing\"):\n",
    "        try:\n",
    "            if file.endswith(\".gz\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                unzip(file_path)\n",
    "        except gzip.BadGzipFile:\n",
    "            print(f\"BadGzipFile: {file}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .tsv file pandas dataframe\n",
    "file_path = fr\"data\\sub-01\\ses-imageryTest01\\func\\sub-01_ses-imageryTest01_task-imagery_run-01_events.tsv\"\n",
    "\n",
    "valid_lines = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Attempt to split the line into columns using tab as the separator\n",
    "            columns = line.strip().split('\\t')\n",
    "            valid_lines.append(columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "# Create a DataFrame from the valid lines\n",
    "tsv_df = pd.DataFrame(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fr\"data\\sub-01\\ses-imageryTest01\\func\\sub-01_ses-imageryTest01_task-imagery_run-01_bold.nii\"\n",
    "# Replace 'your_image.nii' with the path to your NIfTI image file\n",
    "nii_image = nib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = nii_image.get_fdata()\n",
    "\n",
    "print(image_data.shape)\n",
    "# print the middle pixel in rgb#\n",
    "print(image_data[45, 45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo una immagine ogni tre secondi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the header of the NIfTI file\n",
    "header = nii_image.header\n",
    "\n",
    "# Extract the dimension information from the header\n",
    "dimensions = header.get_data_shape()\n",
    "\n",
    "# The number of images is typically the last dimension\n",
    "num_images = dimensions[-1]\n",
    "\n",
    "print(f\"Number of 3D images in the NIfTI file: {num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image data\n",
    "image_data = nii_image.get_fdata()\n",
    "print(image_data.shape)\n",
    "# Create a 2D plot using matplotlib\n",
    "plt.imshow(image_data[:, :, 25, 25])\n",
    "plt.title('Slice of 3D NIfTI Image')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open dimensional image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open34D(nii_image_path, selected_volume=0):\n",
    "    # Load the NIfTI image\n",
    "    nii_image = nib.load(nii_image_path)\n",
    "\n",
    "    # Get the image data as a NumPy array\n",
    "    img_data = nii_image.get_fdata()\n",
    "    shape = img_data.shape\n",
    "    if len(shape) == 4:\n",
    "        img_data = img_data[:, :, :, selected_volume]\n",
    "    else: \n",
    "        img_data = img_data[:, :, 86:]\n",
    "\n",
    "\n",
    "    # Check the data dimensions\n",
    "    if img_data.ndim != 3:\n",
    "        raise ValueError(\"The NIfTI image should be 3D, but it has {} dimensions.\".format(img_data.ndim))\n",
    "\n",
    "    # Check the data type (Mayavi prefers floating-point data)\n",
    "    if img_data.dtype != np.float32:\n",
    "        img_data = img_data.astype(np.float32)\n",
    "\n",
    "    # Create a Mayavi volume visualization\n",
    "    src = mlab.pipeline.scalar_field(img_data)\n",
    "    vol = mlab.pipeline.volume(src)\n",
    "\n",
    "    # Customize the visualization (optional)\n",
    "    vol._volume_property.shade = False  # Enable shading\n",
    "    vol._volume_property.diffuse = 0.7  # Adjust diffuse lighting\n",
    "    vol._volume_property.specular = 0.2  # Adjust specular lighting\n",
    "\n",
    "    # Show the visualization\n",
    "    mlab.show()\n",
    "    print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open2D(file_path, position=50):\n",
    "    # Get the image data\n",
    "    nii_image = nib.load(file_path)\n",
    "    image_data = nii_image.get_fdata()\n",
    "    \n",
    "    shape = nii_image.header.get_data_shape()\n",
    "    print(shape)\n",
    "    if len(shape) == 2:\n",
    "        plt.imshow(image_data)\n",
    "        plt.title('Slice of 3D Image')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    elif len(shape) == 3:\n",
    "        plt.imshow(image_data[:, :, position])\n",
    "        plt.title('Slice of 3D Image')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    elif len(shape) == 4:\n",
    "        image_data = image_data[:, :, :, position]\n",
    "        plt.imshow(image_data[:, :, image_data.shape[2]//2])\n",
    "        plt.title('Slice of 3D Image')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"error, shape = \", len(shape))\n",
    "    print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open2D(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open34D(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semplification of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primo sfoltimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_nii_tsv(path, x):\n",
    "    c = 0\n",
    "    # per ogni file all'interno della cartella path somma tutti i .nii e crea un unico file .nii\n",
    "    every_nii = []\n",
    "    count= 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".nii\"):\n",
    "                nii_image = nib.load(os.path.join(root, file))\n",
    "                image_data = nii_image.get_fdata()\n",
    "                print(\"single .nii file shape:   \", image_data.shape)\n",
    "                print(\"upper file path\", file)\n",
    "                count += image_data.shape[3]\n",
    "                every_nii.append(image_data)\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "    every_nii = np.array(every_nii)\n",
    "    desired_shape = (64, 64, 50, count)\n",
    "    print(\"desired shape :  \", desired_shape)\n",
    "\n",
    "    # Create an empty array to store the sum\n",
    "    result = np.zeros(desired_shape, dtype=np.float32)\n",
    "\n",
    "    # Sum the 3D images along the fourth dimension\n",
    "    for i in range(len(every_nii)):\n",
    "        imgs = every_nii[i]\n",
    "        shape = imgs.shape[3]\n",
    "        for j in range(shape):\n",
    "            result[:, :, :, c] = imgs[:, :, :, j]\n",
    "            c += 1\n",
    "\n",
    "    # Now result should have the sum of all images\n",
    "    print(\"result:   \", result.shape)\n",
    "\n",
    "    result = result.astype(np.float32)\n",
    "    np.save(fr\"{path}\\data{x}.npy\", result)\n",
    "\n",
    "\n",
    "    # sommare files .tsv\n",
    "    valid_lines = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        tsv_files = [file for file in files if file.endswith(\".tsv\")]\n",
    "        tsv_files.sort()  # Ordina i file in base ai loro nomi\n",
    "        for file in tsv_files:\n",
    "            print(\"tsv file \", file)\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Attempt to split the line into columns using tab as the separator\n",
    "                        columns = line.strip().split('\\t')\n",
    "                        # se inizia con onset togli la riga\n",
    "                        if columns[0] == \"onset\":\n",
    "                            continue\n",
    "                        else:\n",
    "                            valid_lines.append(columns)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping line due to error: {e}\")\n",
    "            os.remove(os.path.join(root, file))\n",
    "\n",
    "\n",
    "    every_tsv= pd.DataFrame(valid_lines)\n",
    "\n",
    "    # save .csv\n",
    "    every_tsv.to_csv(fr\"{path}\\data{x}.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lines = []\n",
    "for x in range(1, 4):\n",
    "    path = fr\"data\\sub-01\\ses-imageryTest0{x}\\func\"\n",
    "\n",
    "    sum_nii_tsv(path, x)\n",
    "    \n",
    "    with open(f'{path}\\data{x}.csv', 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            try:\n",
    "                # Attempt to split the line into columns using tab as the separator\n",
    "                columns = line.strip().split('\\t')\n",
    "                valid_lines.append(columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "\n",
    "every_tsv = pd.DataFrame(valid_lines)\n",
    "# Save the final summed DataFrame to a new CSV file\n",
    "every_tsv.to_csv(fr\"data\\sub-01\\ses-imageryTest.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lines = []\n",
    "for x in range(1, 5):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTest0{x}\\func\"\n",
    "    sum_nii_tsv(path, x)\n",
    "    \n",
    "    with open(f'{path}\\data{x}.csv', 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            try:\n",
    "                # Attempt to split the line into columns using tab as the separator\n",
    "                columns = line.strip().split('\\t')\n",
    "                valid_lines.append(columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "\n",
    "every_tsv = pd.DataFrame(valid_lines)\n",
    "# Save the final summed DataFrame to a new CSV file\n",
    "every_tsv.to_csv(fr\"data\\sub-01\\ses-perceptionTest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lines = []\n",
    "for x in range(1, 4):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTraining0{x}\\func\"\n",
    "    sum_nii_tsv(path, x)\n",
    "    \n",
    "    with open(f'{path}\\data{x}.csv', 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            try:\n",
    "                # Attempt to split the line into columns using tab as the separator\n",
    "                columns = line.strip().split('\\t')\n",
    "                valid_lines.append(columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line due to error: {e}\")\n",
    "\n",
    "\n",
    "every_tsv = pd.DataFrame(valid_lines)\n",
    "# Save the final summed DataFrame to a new CSV file\n",
    "every_tsv.to_csv(fr\"data\\sub-01\\ses-perceptionTraining.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x in range(3):\n",
    "    path = fr\"data\\sub-01\\ses-imageryTest0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    print(data.shape)\n",
    "    count += data.shape[3]\n",
    "empty = np.zeros((64, 64, 50, count), dtype=np.float32)\n",
    "print(empty.shape)\n",
    "c = 0\n",
    "for x in range(3):\n",
    "    path = fr\"data\\sub-01\\ses-imageryTest0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    for i in range(data.shape[3]):\n",
    "        empty[:, :, :, c] = data[:, :, :, i]\n",
    "        c += 1\n",
    "np.save(fr\"data\\sub-01\\ses-imageryTest.npy\", empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x in range(4):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTest0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    print(data.shape)\n",
    "    count += data.shape[3]\n",
    "empty = np.zeros((64, 64, 50, count), dtype=np.float32)\n",
    "print(empty.shape)\n",
    "c = 0\n",
    "for x in range(4):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTest0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    for i in range(data.shape[3]):\n",
    "        empty[:, :, :, c] = data[:, :, :, i]\n",
    "        c += 1\n",
    "\n",
    "np.save(fr\"data\\sub-01\\ses-perceptionTest.npy\", empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x in range(3):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTraining0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    print(data.shape)\n",
    "    count += data.shape[3]\n",
    "empty = np.zeros((64, 64, 50, count), dtype=np.float32)\n",
    "print(empty.shape)\n",
    "c = 0\n",
    "for x in range(3):\n",
    "    path = fr\"data\\sub-01\\ses-perceptionTraining0{x+1}\\func\\data{x+1}.npy\"\n",
    "    data = np.load(path)\n",
    "    for i in range(data.shape[3]):\n",
    "        empty[:, :, :, c] = data[:, :, :, i]\n",
    "        c += 1\n",
    "np.save(fr\"data\\sub-01\\ses-perceptionTraining.npy\", empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset\tduration\ttrial_no\tevent_type\tcategory_id\tcategory_name\tcategory_index\tresponse_time\tevaluation\n",
    "\n",
    "header_imagery = [\"onset\", \"duration\", \"trial_no\", \"event_type\", \"category_id\", \"category_name\", \"category_index\", \"response_time\", \"evaluation\"]\n",
    "# onset\tduration\ttrial_no\tevent_type\tstimulus_id\tstimulus_name\tcategory_index\timage_index\tresponse_time\n",
    "header_perception = [\"onset\", \"duration\", \"trial_no\", \"event_type\", \"stimulus_id\", \"stimulus_name\", \"category_index\", \"image_index\", \"response_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagery = pd.read_csv(fr\"data\\sub-01\\ses-imageryTest.csv\", header=None)\n",
    "imagery = imagery.iloc[1:]\n",
    "imagery.columns = header_imagery\n",
    "# save\n",
    "imagery.to_csv(fr\"data\\sub-01\\ses-imageryTest.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perception = pd.read_csv(fr\"data\\sub-01\\ses-perceptionTest.csv\", header=None)\n",
    "# perception = perception.iloc[1:]\n",
    "perception.columns = header_perception\n",
    "perception.to_csv(fr\"data\\sub-01\\ses-perceptionTest.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(fr\"data\\sub-01\\ses-perceptionTraining.csv\", header=None)\n",
    "# training = training.iloc[1:]\n",
    "training.columns = header_perception\n",
    "training.to_csv(fr\"data\\sub-01\\ses-perceptionTraining.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(fr\"data\\sub-01\\ses-imageryTest.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 2d image\n",
    "plt.imshow(test[:, :, 25, 4259])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allineamento immagini con scansioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allineare immagine brain e immagine 2dtrasformali in array numpy\n",
    "# formato immagine 2d: Un array 4D con le dimensioni (numero_di_campioni, altezza, larghezza, canali).\n",
    "# formato immagine brain: Un array 4D con le dimensioni (numero_di_campioni, altezza, larghezza, profonditÃ ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCSV = fr\"data\\sub-01\\ses-perceptionTraining.csv\"\n",
    "df = pd.read_csv(pathCSV, index_col=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = fr\"data\\images\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageBrain(brains, df, x):\n",
    "    durata = 0\n",
    "    brain = brains[:, :, :, x]  \n",
    "    x = x*3\n",
    "    for index in range(df.shape[0]):\n",
    "        try:\n",
    "            duration = df[\"duration\"][index]\n",
    "            duration = int(duration)\n",
    "            durata += duration\n",
    "            if durata > x:\n",
    "                stimolo = df[\"stimulus_name\"][index]\n",
    "                break\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            continue\n",
    "    for root, dirs, files in os.walk(images):\n",
    "        for file in files:\n",
    "            if file.startswith(stimolo):\n",
    "                print(\"file: \", file)\n",
    "                # return a numpy image\n",
    "                image_np = plt.imread(os.path.join(root, file))\n",
    "                image_np = cv2.resize(image_np, (64, 64))\n",
    "                break\n",
    "    return image_np, brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproFinale(pathBrain, pathCSV):\n",
    "    test = np.load(pathBrain)\n",
    "    print(test.shape)\n",
    "    brains_vari = np.zeros((64, 64, 50, test.shape[3]), dtype=np.float32)\n",
    "    features_vari = np.zeros((64, 64, 3, test.shape[3]), dtype=np.float32)\n",
    "    #64*64*50*batch_size\n",
    "    c = 0\n",
    "    df = pd.read_csv(pathCSV, index_col=False, sep=\",\")\n",
    "\n",
    "    for x in range(test.shape[3]):\n",
    "        try:\n",
    "            print(test.shape[3])\n",
    "            feature, brain = ImageBrain(test, df, x)\n",
    "            print(feature.shape)\n",
    "            brains_vari[:, :, :, x] = brain\n",
    "            features_vari[:, :, :, x] = feature\n",
    "            c = c+1\n",
    "        except:\n",
    "            print(\"typeError\")\n",
    "            continue\n",
    "\n",
    "    print(c)\n",
    "\n",
    "    return brains_vari, features_vari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains_path = fr\"data\\sub-01\\ses-perceptionTraining.npy\"\n",
    "csv_path = fr\"data\\sub-01\\ses-perceptionTraining.csv\"\n",
    "brains_vari, features_vari = preproFinale(brains_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains_vari = np.transpose(brains_vari, (3, 0, 1, 2))\n",
    "brains_vari.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vari = np.transpose(features_vari, (3, 0, 1, 2))\n",
    "features_vari.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data (you can use any appropriate scaling method)\n",
    "brains_vari = brains_vari / 255.0\n",
    "features_vari = features_vari / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(fr\"data\\sub-01\\ses-perceptionTrainingImages.npy\", features_vari)\n",
    "np.save(fr\"data\\sub-01\\ses-perceptionTraining.npy\", brains_vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(fr\"data\\sub-01\\ses-perceptionTrainingImages.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_scans = np.load(fr\"data\\sub-01\\ses-perceptionTraining.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prima prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the encoder-decoder architecture\n",
    "input_img = Input(shape=(64, 64, 50))  # Input shape for brain scans\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point, the encoded representation is (8, 8, 128)\n",
    "\n",
    "# Decoder\n",
    "x = Conv2D(1024, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)  # Output has 3 channels for RGB images\n",
    "# attivazione potrebbe ess. da cambiare\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    # ReduceLROnPlateau(monitor='val_loss', factor=0.1, mode = 'auto', patience=5, min_lr=0.0001, verbose=1)\n",
    "    LearningRateScheduler(lambda epoch: 1e-3 * tf.math.exp(-0.1*(epoch//10)))\n",
    "]\n",
    "\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss=MeanAbsoluteError(), metrics=RootMeanSquaredError())  # You can use other loss functions depending on your data and goals\n",
    "\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(brain_scans, \n",
    "                images, \n",
    "                epochs=50, \n",
    "                batch_size=64, \n",
    "                shuffle=True, \n",
    "                validation_split=0.2,\n",
    "                steps_per_epoch=107,\n",
    "                initial_epoch=0,\n",
    "                callbacks=callbacks\n",
    "                )\n",
    "\n",
    "\n",
    "model_path = fr\"autoencoder.keras\"\n",
    "# Save the model\n",
    "autoencoder.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = fr\"autoencoder.keras\"\n",
    "# Save the model\n",
    "autoencoder.load_weights(model_path)\n",
    "\n",
    "\n",
    "\n",
    "brain_train = brain_scans[100, :, :, :]\n",
    "image_train = images[100, :, :, :]\n",
    "single_brain_scan = np.expand_dims(brain_train, axis=0)\n",
    "reconstructed_train = autoencoder.predict(single_brain_scan)\n",
    "\n",
    "image_train= image_train[:, :, :]\n",
    "\n",
    "image_train = ( image_train * 255).astype(np.uint8)\n",
    "\n",
    "reconstructed_image = reconstructed_train[0, :, :, :]\n",
    "\n",
    "reconstructed_image = (reconstructed_image * 255).astype(np.uint8)\n",
    "\n",
    "print(image_train.shape)\n",
    "\n",
    "print(reconstructed_image.shape)\n",
    "\n",
    "\n",
    "Image.fromarray(image_train, mode=\"RGB\").save(\"img.png\")\n",
    "\n",
    "Image.fromarray(reconstructed_image, mode=\"RGB\").save(\"img2.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seconda prova "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "images = np.load(fr\"ses-perceptionTrainingImages.npy\")\n",
    "brain_scans = np.load(fr\"ses-perceptionTraining-001.npy\")\n",
    "\n",
    "# encoder-decoder architecture with residual connections and batch normalization\n",
    "def build_autoencoder_with_residual():\n",
    "    input_img = Input(shape=(64, 64, 50))\n",
    "    \n",
    "    # encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    residual_1 = x  # residual connection\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    residual_2 = x  # residual connection\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # decoder\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.Add()([x, residual_2])\n",
    "    x += residual_2\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.Add()([x, residual_1])\n",
    "    x += residual_1\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "autoencoder_with_residual = build_autoencoder_with_residual()\n",
    "autoencoder_with_residual.compile(optimizer='sgd', loss='mean_squared_error', metrics=RootMeanSquaredError())\n",
    "\n",
    "# callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, mode = 'auto', patience=5, min_lr=0.0001, verbose=1),\n",
    "    LearningRateScheduler(lambda epoch: 1e-3 * tf.math.exp(-0.1*(epoch//10)))\n",
    "]\n",
    "\n",
    "# training\n",
    "autoencoder_with_residual.fit(brain_scans, images, \n",
    "                              epochs=50, batch_size=32, \n",
    "                              shuffle=True,\n",
    "                              validation_split=0.2)\n",
    "\n",
    "model_path = fr\"autoencoder.keras\"\n",
    "# Save the model\n",
    "autoencoder_with_residual.save_weights(model_path)\n",
    "\n",
    "# loss history\n",
    "awr = autoencoder_with_residual\n",
    "plt.plot(awr.history['loss'])\n",
    "plt.plot(awr.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = fr\"autoencoder.keras\"\n",
    "# Save the model\n",
    "autoencoder_with_residual.load_weights(model_path)\n",
    "\n",
    "\n",
    "\n",
    "brain_train = brain_scans[1000, :, :, :]\n",
    "image_train = images[1000, :, :, :]\n",
    "single_brain_scan = np.expand_dims(brain_train, axis=0)\n",
    "reconstructed_train = autoencoder_with_residual.predict(single_brain_scan)\n",
    "\n",
    "image_train= image_train[:, :, :]\n",
    "\n",
    "image_train = ( image_train * 255).astype(np.uint8)\n",
    "\n",
    "reconstructed_image = reconstructed_train[0, :, :, :]\n",
    "\n",
    "reconstructed_image = (reconstructed_image * 255).astype(np.uint8)\n",
    "\n",
    "print(image_train.shape)\n",
    "\n",
    "print(reconstructed_image.shape)\n",
    "\n",
    "\n",
    "Image.fromarray(image_train, mode=\"RGB\").save(\"img.png\")\n",
    "\n",
    "Image.fromarray(reconstructed_image, mode=\"RGB\").save(\"img2.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
